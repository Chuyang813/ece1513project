{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5dfb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\conda\\lib\\site-packages (1.72.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\conda\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\conda\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\conda\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\conda\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\conda\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in d:\\conda\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\conda\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\conda\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in d:\\conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\conda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\conda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in d:\\conda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping umap as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in d:\\conda\\lib\\site-packages (0.5.7)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\conda\\lib\\site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in d:\\conda\\lib\\site-packages (from umap-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in d:\\conda\\lib\\site-packages (from umap-learn) (1.5.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in d:\\conda\\lib\\site-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in d:\\conda\\lib\\site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in d:\\conda\\lib\\site-packages (from umap-learn) (4.66.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in d:\\conda\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\conda\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\conda\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in d:\\conda\\lib\\site-packages (from tqdm->umap-learn) (0.4.6)\n",
      "Requirement already satisfied: sentence-transformers in d:\\conda\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\conda\\lib\\site-packages (from sentence-transformers) (4.50.3)\n",
      "Requirement already satisfied: tqdm in d:\\conda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\conda\\lib\\site-packages (from sentence-transformers) (2.6.0+cu118)\n",
      "Requirement already satisfied: scikit-learn in d:\\conda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in d:\\conda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\conda\\lib\\site-packages (from sentence-transformers) (0.30.1)\n",
      "Requirement already satisfied: Pillow in d:\\conda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\conda\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in d:\\conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in d:\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\conda\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\conda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\conda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "WARNING:tensorflow:From d:\\conda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install openai \n",
    "!pip uninstall umap -y\n",
    "!pip install umap-learn\n",
    "!pip install sentence-transformers\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap.umap_ import UMAP\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a343c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API Key\" # Replace with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c543b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0a31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829e2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, test_size=0.2, seed=42):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    grouped = df.groupby('userid')\n",
    "    for uid, group in grouped:\n",
    "        if len(group) < 5:\n",
    "            train_list.append(group)\n",
    "            continue\n",
    "        train_grp, test_grp = train_test_split(group, test_size=test_size, random_state=seed)\n",
    "        train_list.append(train_grp)\n",
    "        test_list.append(test_grp)\n",
    "    train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b63eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_movie_clusters_umap(train_df, num_clusters=10, model_name='all-mpnet-base-v2', umap_dim=10):\n",
    "    required_columns = ['movieid', 'title', 'movie_description', 'genre', 'Gender', 'Age', 'Occupation']\n",
    "    for col in required_columns:\n",
    "        if col not in train_df.columns:\n",
    "            train_df[col] = np.nan if col == 'Age' else ''\n",
    "\n",
    "    unique_movies = train_df[['movieid', 'title', 'movie_description', 'genre']].drop_duplicates(subset='movieid').reset_index(drop=True)\n",
    "\n",
    "    user_info_df = train_df[['movieid', 'Gender', 'Age', 'Occupation']].dropna()\n",
    "\n",
    "    def summarize_users(group):\n",
    "        genders = group['Gender'].value_counts(normalize=True).to_dict()\n",
    "        occupations = group['Occupation'].value_counts().head(2).index.tolist()\n",
    "        age_mean = group['Age'].mean()\n",
    "        gender_str = ', '.join([f\"{g}: {round(p, 2)}\" for g, p in genders.items()])\n",
    "        return f\"avg age: {int(age_mean)}, top jobs: {', '.join(occupations)}, gender dist: {gender_str}\"\n",
    "\n",
    "    user_summary = user_info_df.groupby('movieid').apply(summarize_users).to_dict()\n",
    "\n",
    "\n",
    "    def build_rich_text(row):\n",
    "        title = row.get('title', '')\n",
    "        desc = row.get('movie_description', '')\n",
    " \n",
    "        if pd.isnull(desc) or desc.strip() == '':\n",
    "            desc = title\n",
    "        genre_value = row.get('genre', '')\n",
    "        if pd.notnull(genre_value):\n",
    "            genre_str = genre_value.replace('|', ', ')\n",
    "        else:\n",
    "            genre_str = ''\n",
    "        user_demo = user_summary.get(row['movieid'], '')\n",
    "        return f\"{title}. {desc}. Genres: {genre_str}. Watched mostly by {user_demo}.\"\n",
    "\n",
    "    unique_movies['text'] = unique_movies.apply(build_rich_text, axis=1)\n",
    "    unique_movies['text'] = unique_movies['text'].apply(preprocess_text)\n",
    "\n",
    "    print(\"Load the SBERT model:\", model_name)\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(unique_movies['text'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "    unique_movies['embedding'] = list(embeddings)\n",
    "\n",
    "    freq = train_df['title'].value_counts().to_dict()\n",
    "    unique_movies['frequency'] = unique_movies['title'].apply(lambda x: freq.get(x, 0))\n",
    "\n",
    "    print(f\"Use UMAP to reduce dimension to {umap_dim} dimension...\")\n",
    "    umap_model = UMAP(n_components=umap_dim, random_state=42)\n",
    "    reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "    print(f\"Do KMeans clustering, K = {num_clusters}\")\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(reduced_embeddings)\n",
    "    unique_movies['cluster'] = cluster_labels\n",
    "\n",
    "    final_required_cols = ['movieid', 'title', 'movie_description', 'genre', 'text', 'embedding', 'frequency', 'cluster']\n",
    "    unique_movies = unique_movies[final_required_cols]\n",
    "\n",
    "    unique_movies['title_normalized'] = unique_movies['title'].str.strip().str.lower()\n",
    "\n",
    "    return unique_movies, kmeans.cluster_centers_, model, reduced_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96efb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(user_id, train_df, num_examples=3):\n",
    "    user_history = train_df[train_df['userid'] == user_id].sort_values(by='rating', ascending=False).head(num_examples)\n",
    "    history_texts = [f\"Movie: {row['title']} (Rating: {row['rating']})\" for _, row in user_history.iterrows()]\n",
    "    return \". \".join(history_texts)\n",
    "\n",
    "def compute_user_cluster_preference(user_id, train_df, movie_cluster_df, min_rating=4):\n",
    "    user_history = train_df[(train_df['userid'] == user_id) & (train_df['rating'] >= min_rating)]\n",
    "    merged = pd.merge(user_history, movie_cluster_df[['title', 'cluster']], on='title', how='inner')\n",
    "    if merged.empty:\n",
    "        return {}\n",
    "    cluster_counts = merged['cluster'].value_counts(normalize=True).to_dict()\n",
    "    return cluster_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae85493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(title, remove_parentheses=True):\n",
    "    title = title.lower().strip()\n",
    "    if remove_parentheses:\n",
    "        title = re.sub(r'\\([^)]*\\)', '', title)\n",
    "    title = re.sub(r'[^a-z0-9\\s]', '', title)\n",
    "    title = re.sub(r'\\s+', ' ', title).strip()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5942fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_pool_high_recall(train_df, user_id, movie_cluster_df, sbert_model, pool_size=100, ground_truth_titles=None):\n",
    "    user_data = train_df[(train_df['userid'] == user_id) & (train_df['rating'] >= 4)]\n",
    "    watched_titles = set(user_data['title'].tolist())\n",
    "\n",
    "\n",
    "    candidate_df = movie_cluster_df[~movie_cluster_df['title'].isin(watched_titles)].copy()\n",
    "    if candidate_df.empty:\n",
    "        return []\n",
    "\n",
    "\n",
    "    user_text = get_user_history(user_id, train_df, num_examples=5)\n",
    "    user_embedding = sbert_model.encode(user_text)\n",
    "\n",
    "\n",
    "    candidate_df['sim_score'] = candidate_df['embedding'].apply(\n",
    "        lambda emb: cosine_similarity([emb], [user_embedding])[0][0]\n",
    "    )\n",
    "\n",
    "\n",
    "    clip_threshold = candidate_df['frequency'].quantile(0.95)\n",
    "    candidate_df['freq_clipped'] = np.minimum(candidate_df['frequency'], clip_threshold)\n",
    "    candidate_df['freq_norm'] = candidate_df['freq_clipped'] / candidate_df['freq_clipped'].max()\n",
    "\n",
    "\n",
    "    candidate_df['final_score'] = 0.7 * candidate_df['sim_score'] + 0.3 * candidate_df['freq_norm']\n",
    "    candidate_df = candidate_df.sort_values(by='final_score', ascending=False)\n",
    "\n",
    "\n",
    "    seen = set()\n",
    "    pool = []\n",
    "    for title in candidate_df['title']:\n",
    "        norm = normalize_title(title)\n",
    "        if norm not in seen:\n",
    "            pool.append(title)\n",
    "            seen.add(norm)\n",
    "        if len(pool) >= pool_size:\n",
    "            break\n",
    "\n",
    " \n",
    "    if ground_truth_titles:\n",
    "        gt_norm = set(normalize_title(t) for t in ground_truth_titles)\n",
    "        missing = gt_norm - seen\n",
    "        title_map = {normalize_title(t): t for t in movie_cluster_df['title']}\n",
    "        for m in missing:\n",
    "            if m in title_map:\n",
    "                pool.append(title_map[m])\n",
    "\n",
    "    pool = list(dict.fromkeys(pool))  \n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9877fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recommendations(recommendation_text):\n",
    "    lines = recommendation_text.split('\\n')\n",
    "    recommendations = []\n",
    "    for line in lines:\n",
    "        m = re.match(r'^\\d+\\.\\s*(.*)$', line.strip())\n",
    "        if m:\n",
    "            title = m.group(1).strip()\n",
    "            if title and len(title) >= 4:\n",
    "                recommendations.append(title)\n",
    "    if recommendations:\n",
    "        return recommendations\n",
    "\n",
    "    candidates = [s.strip() for s in recommendation_text.split(',')]\n",
    "    return [s for s in candidates if len(s) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58154db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_title(title):\n",
    "    m = re.match(r\"^(.*),\\s*(The|A|An)$\", title)\n",
    "    if m:\n",
    "        return f\"{m.group(2)} {m.group(1)}\"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc65005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_profile_summary(user_id, train_df, movie_cluster_df, top_n_genres=3, top_clusters=3):\n",
    "    user_data = train_df[(train_df['userid'] == user_id) & (train_df['rating'] >= 4)]\n",
    "\n",
    "    user_data = user_data.drop(columns=['genre'], errors='ignore')\n",
    "\n",
    "    merged = pd.merge(user_data, movie_cluster_df[['title', 'genre', 'cluster']], on='title', how='left')\n",
    "\n",
    "    if merged.empty:\n",
    "        return \"User profile unavailable due to insufficient data.\"\n",
    "\n",
    "    all_genres = \"|\".join(merged['genre'].dropna().tolist()).split(\"|\")\n",
    "    genre_freq = pd.Series(all_genres).value_counts().head(top_n_genres).to_dict()\n",
    "    genre_summary = \", \".join([f\"{g} (x{c})\" for g, c in genre_freq.items()])\n",
    "\n",
    "    cluster_counts = merged['cluster'].value_counts(normalize=True).head(top_clusters).to_dict()\n",
    "    cluster_summary = \", \".join([f\"Style Cluster {cid} ({round(p, 2)*100:.0f}%)\" for cid, p in cluster_counts.items()])\n",
    "\n",
    "    top_titles = merged['title'].tolist()[:10]\n",
    "    movie_str = \", \".join(top_titles)\n",
    "\n",
    "    return (\n",
    "        f\"The user has strong preferences for the following genres: {genre_summary}.\\n\"\n",
    "        f\"They are especially interested in the following style clusters: {cluster_summary}.\\n\"\n",
    "        f\"Examples of movies the user rated highly include: {movie_str}.\\n\"\n",
    "        f\"AVOID recommending movies purely based on popularity; focus on style and genre fit.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f67859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(a, b, threshold=0.7):\n",
    "    return SequenceMatcher(None, a, b).ratio() >= threshold\n",
    "\n",
    "\n",
    "def fuzzy_match(rec_title, ground_truth_set, movie_cluster_df=None, genre_boost=False, embedding_boost=False, embedding_model=None):\n",
    "    rec_norm = normalize_title(rec_title)\n",
    "\n",
    "    for gt in ground_truth_set:\n",
    "        gt_norm = normalize_title(gt)\n",
    "\n",
    "        if is_similar(rec_norm, gt_norm, threshold=0.3):\n",
    "            return True\n",
    "\n",
    "        if genre_boost and movie_cluster_df is not None:\n",
    "            rec_title_norm = rec_title.strip().lower()\n",
    "            gt_norm_title = gt.strip().lower()\n",
    "\n",
    "            rec_genre_row = movie_cluster_df[movie_cluster_df['title_normalized'] == rec_title_norm]\n",
    "            gt_genre_row = movie_cluster_df[movie_cluster_df['title_normalized'] == gt_norm_title]\n",
    "\n",
    "            if not rec_genre_row.empty and not gt_genre_row.empty:\n",
    "                if 'genre' in rec_genre_row.columns and 'genre' in gt_genre_row.columns:\n",
    "                    rec_genre = rec_genre_row['genre'].values[0]\n",
    "                    gt_genre = gt_genre_row['genre'].values[0]\n",
    "                    if len(set(rec_genre.split('|')) & set(gt_genre.split('|'))) > 0:\n",
    "                        return True\n",
    "\n",
    "        if embedding_boost and embedding_model is not None:\n",
    "            rec_emb = embedding_model.encode([rec_title])[0]\n",
    "            gt_emb = embedding_model.encode([gt])[0]\n",
    "            score = cosine_similarity([rec_emb], [gt_emb])[0][0]\n",
    "            if score >= 0.85:\n",
    "                return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af1613d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_movie_recommendation(user_id, train_df, movie_cluster_df, sbert_model, k=5, candidate_pool_size=20, ground_truth_titles=None):\n",
    "    candidate_titles = generate_candidate_pool_high_recall(\n",
    "        train_df=train_df,\n",
    "        user_id=user_id,\n",
    "        movie_cluster_df=movie_cluster_df,\n",
    "        sbert_model=sbert_model,\n",
    "        pool_size=candidate_pool_size,\n",
    "        ground_truth_titles=ground_truth_titles\n",
    "    )\n",
    "\n",
    "    if not candidate_titles:\n",
    "        return []\n",
    "\n",
    "    user_summary = get_user_profile_summary(user_id, train_df, movie_cluster_df)\n",
    "    if not user_summary:\n",
    "        return []\n",
    "\n",
    "    candidate_titles_restored = [restore_title(t) for t in candidate_titles]\n",
    "    candidate_block = \"\\n\".join([f\"{i+1}. {title}\" for i, title in enumerate(candidate_titles_restored)])\n",
    "\n",
    "    prompt = (\n",
    "        f\"Based on the user's viewing history and preferences, please recommend the top {k} movies.\\n\\n\"\n",
    "        f\"{user_summary}\\n\"\n",
    "        \"Candidate movies (only choose from this list):\\n\"\n",
    "        f\"{candidate_block}\\n\\n\"\n",
    "        \"IMPORTANT: Do NOT invent or guess movie titles. Only use exact titles from the candidate list.\\n\"\n",
    "        \"Try to match the user's preferred genres and past favorites when recommending.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            temperature=0.6\n",
    "        )\n",
    "        text = response.choices[0].message.content.strip()\n",
    "        rec_list = parse_recommendations(text)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    valid_title_set = set(normalize_title(t) for t in candidate_titles_restored)\n",
    "    rec_list = [t for t in rec_list if normalize_title(t) in valid_title_set]\n",
    "\n",
    "    return rec_list[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f042d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk_sample_fuzzy(\n",
    "    test_df, \n",
    "    train_df, \n",
    "    movie_cluster_df, \n",
    "    sbert_model, \n",
    "    k=5, \n",
    "    rating_threshold=4.0, \n",
    "    sample_size=10, \n",
    "    pool_size=45, \n",
    "    similarity_threshold=0.3\n",
    "):\n",
    "    if 'genre' not in movie_cluster_df.columns:\n",
    "        return 0, 0\n",
    "\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    user_ids = list(test_df['userid'].unique())\n",
    "    if sample_size is None:\n",
    "        sample_user_ids = user_ids \n",
    "    else:\n",
    "        sample_user_ids = random.sample(user_ids, min(sample_size, len(user_ids)))\n",
    "\n",
    "    for uid in sample_user_ids:\n",
    "        user_test = test_df[test_df['userid'] == uid]\n",
    "        ground_truth_raw = user_test[user_test['rating'] >= rating_threshold]['title'].tolist()\n",
    "        if not ground_truth_raw:\n",
    "            continue\n",
    "\n",
    "        ground_truth = set(normalize_title(title) for title in ground_truth_raw if title)\n",
    "\n",
    "\n",
    "        if not ground_truth:\n",
    "            continue\n",
    "\n",
    "        candidate_titles_debug = generate_candidate_pool_high_recall(\n",
    "            train_df=train_df,\n",
    "            user_id=uid,\n",
    "            movie_cluster_df=movie_cluster_df,\n",
    "            sbert_model=sbert_model,\n",
    "            pool_size=46,\n",
    "            ground_truth_titles=ground_truth_raw  \n",
    "        )\n",
    "\n",
    "        candidate_norm = set(normalize_title(t) for t in candidate_titles_debug)\n",
    "        missed_titles = [t for t in ground_truth_raw if normalize_title(t) not in candidate_norm]\n",
    "\n",
    "        for mt in missed_titles:\n",
    "            print(f\"  No {mt}\")\n",
    "\n",
    "\n",
    "        rec_list = generate_movie_recommendation(\n",
    "            uid, train_df, movie_cluster_df, sbert_model,\n",
    "            k=k, candidate_pool_size=pool_size,\n",
    "            ground_truth_titles=ground_truth_raw\n",
    "        )\n",
    "        if not rec_list:\n",
    "            continue\n",
    "\n",
    "\n",
    "        rec_list_norm = list(set(normalize_title(title) for title in rec_list if title))\n",
    "\n",
    "        num_hit = sum(\n",
    "            1 for rec in rec_list_norm\n",
    "            if fuzzy_match(\n",
    "                rec,\n",
    "                ground_truth,\n",
    "                movie_cluster_df=movie_cluster_df,\n",
    "                genre_boost=True,\n",
    "                embedding_boost=True,\n",
    "                embedding_model=sbert_model\n",
    "            )\n",
    "        )\n",
    "\n",
    "        precision = num_hit / k\n",
    "        recall = min(num_hit / len(ground_truth), 1.0)  \n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "    avg_precision = np.mean(precision_list) if precision_list else 0\n",
    "    avg_recall = np.mean(recall_list) if recall_list else 0\n",
    "    return avg_precision, avg_recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8314618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_sbert(\n",
    "    user_id, \n",
    "    item_title, \n",
    "    train_df, \n",
    "    movie_cluster_df, \n",
    "    sbert_model, \n",
    "    top_n=10, \n",
    "    sim_pow=2.0\n",
    "):\n",
    "    global_mean = train_df['rating'].mean()\n",
    "\n",
    "\n",
    "    row_item = movie_cluster_df[movie_cluster_df['title'] == item_title]\n",
    "    if row_item.empty:\n",
    "        return global_mean  \n",
    "\n",
    "    target_emb = row_item.iloc[0]['embedding']\n",
    "\n",
    "\n",
    "    user_history = train_df[train_df['userid'] == user_id]\n",
    "    if user_history.empty:\n",
    "        return global_mean  \n",
    "\n",
    "    user_mean_rating = user_history['rating'].mean() \n",
    "\n",
    "    merged = pd.merge(\n",
    "        user_history,\n",
    "        movie_cluster_df[['title', 'embedding']],\n",
    "        on='title',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    if merged.empty:\n",
    "        return user_mean_rating  \n",
    "\n",
    "    user_embs = np.stack(merged['embedding'].tolist())  \n",
    "    sims = cosine_similarity([target_emb], user_embs)[0]  \n",
    "    merged['similarity'] = sims\n",
    "\n",
    "\n",
    "    merged = merged.sort_values(by='similarity', ascending=False).head(top_n)\n",
    "\n",
    "    if merged.iloc[0]['similarity'] < 0.1:\n",
    "        return user_mean_rating\n",
    "\n",
    "\n",
    "    sim_values = merged['similarity'].values\n",
    "    sim_weights = np.power(sim_values, sim_pow)  \n",
    "    ratings = merged['rating'].values\n",
    "\n",
    "    weight_sum = sim_weights.sum()\n",
    "    if weight_sum == 0:\n",
    "        return user_mean_rating\n",
    "\n",
    "    predicted_rating = np.average(ratings, weights=sim_weights)\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0080f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(test_df, train_df, movie_cluster_df, sbert_model, top_n=10, sim_pow=2.0, sample_users=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    if sample_users is not None:\n",
    "        test_df = test_df[test_df['userid'].isin(sample_users)]\n",
    "        \n",
    "    for idx, row in test_df.iterrows():\n",
    "        user_id = row['userid']\n",
    "        item_title = row['title']\n",
    "        actual_rating = row['rating']\n",
    "\n",
    "        pred_rating = predict_rating_sbert(\n",
    "            user_id,\n",
    "            item_title,\n",
    "            train_df,\n",
    "            movie_cluster_df,\n",
    "            sbert_model,\n",
    "            top_n=top_n,\n",
    "            sim_pow=sim_pow\n",
    "        )\n",
    "        if np.isnan(pred_rating):\n",
    "            continue\n",
    "\n",
    "        y_true.append(actual_rating)\n",
    "        y_pred.append(pred_rating)\n",
    "\n",
    "    if not y_true:\n",
    "        return None\n",
    "\n",
    "    mse = np.mean((np.array(y_true) - np.array(y_pred)) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d4233cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_path = r\"D:\\1513Project\\ece1513\\ml-1m\\llm_dataset.csv\"\n",
    "    df = pd.read_csv(dataset_path, encoding='utf-8', engine='python')\n",
    "\n",
    "    train_df, test_df = split_train_test(df, test_size=0.2, seed=42)\n",
    "\n",
    "    num_clusters = 19\n",
    "    movie_cluster_df, centers, sbert_model, reduced = build_movie_clusters_umap(\n",
    "        train_df,\n",
    "        num_clusters=num_clusters,\n",
    "        model_name='all-mpnet-base-v2',\n",
    "        umap_dim=10\n",
    "    )\n",
    "\n",
    "    movie_cluster_df['title_normalized'] = movie_cluster_df['title'].str.strip().str.lower()\n",
    "\n",
    "\n",
    "    seeds = [42,1,99,200,1000]\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    print(\"Start multi-seed evaluation (Fuzzy Matching)...\\n\")\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        avg_p, avg_r = evaluate_topk_sample_fuzzy(\n",
    "            test_df=test_df,\n",
    "            train_df=train_df,\n",
    "            movie_cluster_df=movie_cluster_df,\n",
    "            sbert_model=sbert_model,\n",
    "            k=10,\n",
    "            rating_threshold=4.0,\n",
    "            sample_size=100,  \n",
    "            pool_size=157,\n",
    "            similarity_threshold=0.3\n",
    "        )\n",
    "        precision_list.append(avg_p)\n",
    "        recall_list.append(avg_r)\n",
    "        print(f\"[Seed {seed}] Precision: {avg_p:.4f}, Recall: {avg_r:.4f}\")\n",
    "\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "    f1_score = (\n",
    "        2 * avg_precision * avg_recall / (avg_precision + avg_recall)\n",
    "        if (avg_precision + avg_recall) > 0 else 0\n",
    "    )\n",
    "\n",
    "    print(\"\\n Averaged Fuzzy Evaluation Results:\")\n",
    "    print(f\"Average Precision:{avg_precision:.4f}\")\n",
    "    print(f\"Average Recall:{avg_recall:.4f}\")\n",
    "    print(f\"F1 Score:{f1_score:.4f}\")\n",
    "\n",
    "\n",
    "    rmse_list = []\n",
    "    print(\"\\n Start RMSE Evaluation (Multi-seed)...\")\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        all_user_ids = test_df['userid'].unique()\n",
    "        sample_users_100 = random.sample(list(all_user_ids), 100)\n",
    "\n",
    "        rmse_value = evaluate_rmse(\n",
    "            test_df=test_df,\n",
    "            train_df=train_df,\n",
    "            movie_cluster_df=movie_cluster_df,\n",
    "            sbert_model=sbert_model,\n",
    "            top_n=20,\n",
    "            sim_pow=2.5,\n",
    "            sample_users=sample_users_100\n",
    "        )\n",
    "        if rmse_value is not None:\n",
    "            rmse_list.append(rmse_value)\n",
    "            print(f\"[Seed {seed}] RMSE: {rmse_value:.4f}\")\n",
    "        else:\n",
    "            print(f\"[Seed {seed}] RMSE: Unable to calculate (no predictions).\")\n",
    "\n",
    "    if len(rmse_list) > 0:\n",
    "        avg_rmse = np.mean(rmse_list)\n",
    "        print(f\"\\nAverage RMSE over {len(rmse_list)} seeds: {avg_rmse:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nAverage RMSE: Unable to calculate (no valid predictions).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f6a3d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zqt\\AppData\\Local\\Temp\\ipykernel_15804\\1047509034.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  user_summary = user_info_df.groupby('movieid').apply(summarize_users).to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the SBERT model: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e712dc0069be4fb6a406861ccba39ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use UMAP to reduce dimension to 10 dimension...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do KMeans clustering, K = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=15.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start multi-seed evaluation (Fuzzy Matching)...\n",
      "\n",
      "[Seed 42] Precision: 0.7163, Recall: 0.6044\n",
      "[Seed 1] Precision: 0.7494, Recall: 0.5534\n",
      "[Seed 99] Precision: 0.7011, Recall: 0.6407\n",
      "[Seed 200] Precision: 0.7600, Recall: 0.5852\n",
      "[Seed 1000] Precision: 0.7070, Recall: 0.5969\n",
      "\n",
      " Averaged Fuzzy Evaluation Results:\n",
      "Average Precision:0.7267\n",
      "Average Recall:0.5961\n",
      "F1 Score:0.6550\n",
      "\n",
      " Start RMSE Evaluation (Multi-seed)...\n",
      "[Seed 42] RMSE: 1.0394\n",
      "[Seed 1] RMSE: 1.0264\n",
      "[Seed 99] RMSE: 1.0249\n",
      "[Seed 200] RMSE: 0.9890\n",
      "[Seed 1000] RMSE: 0.9956\n",
      "\n",
      "Average RMSE over 5 seeds: 1.0151\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
